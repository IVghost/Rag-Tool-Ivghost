# document.py

import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

from utils import chunk_text, extract_text_from_pdf
from llm_client import query_llm

logger = logging.getLogger(__name__)


def summarize_chunk(chunk: str, provider: str, model: str, api_key: str) -> str:
    """R√©sume un chunk de texte en ciblant obligations, risques et engagements."""
    prompt = (
        "Voici un extrait de contrat. R√©sume les points cl√©s en identifiant :\n"
        "- Les obligations des parties\n"
        "- Les clauses √† risques\n"
        "- Les engagements r√©ciproques\n\n"
        "Sois synth√©tique, mais rigoureux et utile juridiquement.\n\n"
        f"---\n{chunk}\n---"
    )
    try:
        return query_llm(prompt, model, provider, api_key)
    except Exception as e:
        logger.error(f"Erreur lors du r√©sum√© d‚Äôun chunk : {e}")
        return "[Erreur pendant le r√©sum√© de cette section]"


def analyze_document(file, question, model, full_analysis, provider, api_key, *args):
    """
    - file: le chemin ou objet fichier √† analyser
    - question: texte de la question (utilis√© si full_analysis=False)
    - model: nom du mod√®le LLM
    - full_analysis: bool, True pour analyse juridique d√©taill√©e
    - provider: nom du fournisseur IA (Ollama, OpenAI, etc.)
    - api_key: cl√© API (vide si Ollama local)
    - *args: absorb any extra args from Gradio
    """
    start = time.time()
    logger.info(f"[{time.strftime('%H:%M:%S')}] üîç D√©but de l'analyse du document...")

    # 1. Extraction du texte
    text = extract_text_from_pdf(file)

    # 2. Si on veut juste r√©pondre √† une question
    if not full_analysis:
        if not question:
            return "‚ùå Veuillez fournir une question pour l'analyse."
        prompt = f"Voici un document :\n{text}\n\nQuestion : {question}"
        logger.info("üîÑ Envoi du prompt question au LLM...")
        result = query_llm(prompt, model, provider, api_key)
        elapsed = round(time.time() - start, 2)
        logger.info(f"‚úÖ Analyse termin√©e en {elapsed}s.")
        return result

    # 3. Analyse compl√®te en mode chunk + synth√®se
    logger.info("üîÄ Chunking du document pour r√©sum√© parall√®le...")
    chunks = chunk_text(text, max_tokens=600)

    summaries = []
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(summarize_chunk, chunk, provider, model, api_key): idx
            for idx, chunk in enumerate(chunks)
        }
        for future in as_completed(futures):
            idx = futures[future]
            logger.info(f"üîñ R√©sum√© chunk {idx + 1}/{len(chunks)}")
            try:
                summaries.append(future.result())
            except Exception as e:
                summaries.append("[Erreur dans ce chunk]")
                logger.error(f"Erreur chunk {idx + 1}: {e}")

    combined_summary = "\n\n".join(summaries)

    # 4. Synth√®se finale
    final_prompt = (
        "√Ä partir des r√©sum√©s ci-dessous d‚Äôun contrat, r√©alise une synth√®se claire :\n"
        "- Liste les obligations principales pour chaque partie\n"
        "- Identifie les clauses √† risques ou ambigu√´s\n"
        "- Reformule les engagements mutuels\n\n"
        f"{combined_summary}"
    )
    logger.info("üîÑ Envoi du prompt de synth√®se finale au LLM...")
    result = query_llm(final_prompt, model, provider, api_key)

    elapsed = round(time.time() - start, 2)
    logger.info(f"‚úÖ Analyse compl√®te termin√©e en {elapsed}s.")
    return result
